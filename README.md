# Image Search Demo

A text-to-image search service powered by OpenAI's CLIP model. This service allows users to search for images using natural language queries and provide feedback on search results.

## Features

- ðŸ” Text-to-image search using CLIP embeddings
- ðŸ“Š User feedback collection on search results
- ðŸ—„ï¸ Vector similarity search with pgvector
- ðŸ³ Fully containerized with Docker
- ðŸš€ High-performance async API with FastAPI
- ðŸ“¦ S3-compatible object storage support

## Architecture

The service consists of several components:

- **FastAPI Web Server**: Handles HTTP requests and coordinates between services
- **PostgreSQL + pgvector**: Stores image metadata and vector embeddings
- **MinIO**: S3-compatible object storage for image files
- **CLIP Model**: Generates embeddings for text queries and images

## Quick Start

1. Clone the repository:

> [!IMPORTANT]: Pease ensure git-lfs is installed otherwise the assets will not be downloaded.
> For mac users, you can install it by running `brew install git-lfs && git lfs install`

```bash
git clone https://github.com/yckao/image-search-demo.git
cd image-search-demo
```

2. Create environent file for development

```bash
cp .env.example .env
```

3. Start the services using Docker Compose:

```bash
docker compose -f hack/compose/docker-compose.yaml --profile dev up -d
```

4. [Opitional] Upload assets

```bash
docker compose -f hack/compose/docker-compose.yaml run --rm dev-assets
```

5. Open the API documentation at http://localhost:8000/docs

### Clean up

```bash
docker compose -f hack/compose/docker-compose.yaml --profile dev down -v
```

## Future Work

- [ ] Add tests
- [ ] Add observability (metrics, tracing, logging)
